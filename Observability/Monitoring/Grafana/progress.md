ğŸ“Š Grafana Setup and Configuration
Since Grafana was already included in the Prometheus Helm chart, I only needed to create an Ingress rule to expose it externally, similar to what I did for Prometheus.

ğŸŒ Creating Grafana Ingress
Configuration Details:

Domain: grafana.treecom.site
Path: /
Target Port: 80

Configure DNS Resolution
Added an entry to the hosts file:
192.168.234.130    grafana.treecom.site
Grafana is now accessible at:
http://grafana.treecom.site:30437/

ğŸ” Retrieving Grafana Credentials
The Helm chart generates an autogenerated password for Grafana, which can be retrieved using:
bashkubectl get secret prometheus-grafana -n default -o jsonpath="{.data.admin-password}" | base64 --decode
Login Credentials:

Username: admin
Password: <output from above command>


ğŸ”— Connecting Grafana to Prometheus
After logging into Grafana, I added Prometheus as a data source with the following configuration:
SettingValueNamePrometheusTypePrometheusURLhttp://prometheus-kube-prometheus-prometheus:9090AccessServer (default)AuthNo AuthSkip TLS VerifyOFFScrape interval15sQuery timeout60sHTTP MethodPOST
âœ… Result: Grafana is now connected to Prometheus and receiving data successfully!

ğŸ“ˆ Pre-configured Dashboards
The Helm chart automatically imports 31 pre-defined dashboards from ConfigMaps, providing instant visibility into various cluster metrics.

ğŸ”§ Troubleshooting: Ingress Nginx Dashboard
ğŸ”´ The Problem
When I tried to import the Ingress Nginx Dashboard (ID: 9614), it showed no data.
Root Cause Analysis:

âŒ Ingress controller was not exposing metrics
âŒ Prometheus was not scraping the Ingress controller


âœ… The Solution
1ï¸âƒ£ Enable Metrics on Ingress Controller
Added the following arguments to the Ingress controller deployment:
yamlargs:
  - --enable-metrics=true
  - --metrics-per-host=false
2ï¸âƒ£ Expose Metrics Port on Ingress Service
Added port 10254 to the Ingress controller service:
yamlports:
  - name: metrics
    port: 10254
    protocol: TCP
    targetPort: metrics
3ï¸âƒ£ Create ServiceMonitor
Created a ServiceMonitor resource to configure Prometheus to scrape the Ingress controller metrics.

ğŸ’¡ Key Learning

Important: For Prometheus to monitor any new service, you need to:

âœ… Ensure the service exposes its metrics (usually via a /metrics endpoint)
âœ… Create a ServiceMonitor resource to tell Prometheus where to scrape
âœ… Verify the metrics port is properly exposed on the service


This principle applies to any service you want to monitor with Prometheus, not just Ingress controllers.

I got an issue with the setup that:
Grafana stores all dashboards and settings inside the container at:
/var/lib/grafana/grafana.db

Because this is ephemeral, the data gets deleted if the Grafana pod restarts or moves to another node.
To fix this, I added persistent storage and node affinity so Grafana always uses the same data directory.

Backup Existing Grafana Data:
GRAFANA_POD=$(kubectl get pods -l app.kubernetes.io/name=grafana -o jsonpath='{.items[0].metadata.name}')
kubectl cp $GRAFANA_POD:/var/lib/grafana/grafana.db ./grafana-backup-$(date +%Y%m%d).db

Create PV & PVC:
I created a Persistent Volume and Persistent Volume Claim so Grafana can save its grafana.db on permanent storage instead of inside the container.

Add Node Affinity to Grafana Deployment:
Since the PV is node-local, Grafana must always run on the same worker node.
I added node affinity to ensure Grafana always schedules on worker1.

Restore Backup to PV Location:
sudo scp ./grafana-backup-*.db root@192.168.234.129:/data/grafana/
This restores all previous dashboards and settings onto the new persistent storage.
